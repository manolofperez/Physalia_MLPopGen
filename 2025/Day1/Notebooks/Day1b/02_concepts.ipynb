{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![QMUL](Images/QMUL-logo.jpg)\n",
    "\n",
    "# Introduction to Approximate Bayesian Computation in population genetics\n",
    "\n",
    "## Bayesian concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Bayesian statistics, $\\theta$ is not a fixed (although unknown) parameter\n",
    "but a random quantity.\n",
    "\n",
    "This is done by adopting a probability distribution, called _prior distribution_ , for ${\\theta}$ that contains any information we have about ${\\theta}$ not related to the data ${y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Inferences on ${\\theta}$ are based on its _posterior distribution_ given by\n",
    "\\begin{equation}\n",
    "    P({\\theta}|{y}) = \\frac{f({y}|{\\theta})\\pi({\\theta})}{m({y})} \n",
    "                        = \\frac{f({y}|{\\theta})\\pi({\\theta})}{\\int f({y}|{\\theta}) \\pi({\\theta}) d{\\theta}}\n",
    "\\end{equation}\n",
    "\n",
    "This formula is known as _Bayes' Theorem_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normal/Normal model\n",
    "\n",
    "Let's assume that we monitored a certain number of frogs in a given pond and want to make some inferences on the infection rate, $\\theta$, in the whole area.\n",
    "\n",
    "<img src=\"Images/Muletensis.jpeg\" width=\"500\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If both the prior and the likelihood are Normal (Gaussian) distributions, then \n",
    "\\begin{align}\n",
    " f(y|\\theta) &= N(y|\\theta, \\sigma^2)\\\\\n",
    " \\pi(\\theta) &= N(\\theta|\\mu, \\tau^2)\n",
    "\\end{align}\n",
    "\n",
    "$\\mu$ and $\\tau$ are known _hyperparameters_ while $\\theta$ is the unknown parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The posterior distribution $p(\\theta|y)$ is also a Normal distribution\n",
    "\\begin{equation}\n",
    "  p(\\theta|y) = N(\\theta|\\frac{\\sigma^2\\mu+\\tau^2y}{\\sigma^2+\\tau^2},\\frac{\\sigma^2\\tau^2}{\\sigma^2 + \\tau^2})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If \n",
    "\\begin{equation}\n",
    "  B = \\frac{\\sigma^2}{\\sigma^2+\\tau^2}\n",
    "\\end{equation}\n",
    "then \n",
    "\\begin{align}\n",
    " E(\\theta|y) &= B\\mu + (1-B)y \\\\\n",
    " Var(\\theta|y) &= (1-B)\\sigma^2 \\equiv B\\tau^2\n",
    "\\end{align}\n",
    "\n",
    "$B$ is called the _shrinking factor_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It is called like that because it gives the proportion for how much the posterior mean is \"shrunk back\" from the classical frequentist estimate $y$ towards the prior mean $\\mu$.\n",
    "\n",
    "Note that $0 \\leq B \\leq 1$.\n",
    "\n",
    "The posterior mean is a weighted average of the prior mean $\\mu$ and the direct estimate $Y$.\n",
    "The weight on the prior mean $B$ depends on the relative variability of the prior distribution and the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If $\\sigma^2>>\\tau^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "then $B \\approx 1$ and our prior knowledge is more precise than the data information.\n",
    "\n",
    "If $\\sigma^2<<\\tau^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "then $B \\approx 0$ and our prior knowledge is imprecise and the final estimate will move very little towards\n",
    "the prior mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Images/Muletensis.jpeg\" width=\"300\" height=\"300\" />\n",
    "\n",
    "* We have a single observation from one pond of $6$ infected frogs ($y=6$).\n",
    "\n",
    "* Our likelihood function (Normal distribution) has $\\sigma=1$.\n",
    "\n",
    "* We expected $2$ infected frogs before doing the monitoring (with variance equal to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More formally,\n",
    "\\begin{align}\n",
    " f(y=6|\\theta) &= N(y=6|\\theta, 1) \\\\\n",
    " \\pi(\\theta) &= N(\\theta|2, 1)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# prior\n",
    "mu <- 2\n",
    "tau <- 1\n",
    "\n",
    "x <- seq(-4,10,0.01)\n",
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,0.6),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,0.6),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3) # prior\n",
    "\n",
    "# likelihood\n",
    "y <- 6\n",
    "sigma <- 1\n",
    "points(x=x, y=dnorm(x=y, mean=x, sd=sigma), type=\"l\", lty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# prior\n",
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,0.6),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3) # prior\n",
    "\n",
    "# likelihood\n",
    "points(x=x, y=dnorm(x=y, mean=x, sd=sigma), type=\"l\", lty=2) # likelihood\n",
    "\n",
    "# posterior\n",
    "B <- sigma^2/(sigma^2+tau^2)\n",
    "postMean <- B*mu + (1-B)*y\n",
    "postVar <- B*tau^2\n",
    "points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The prior distribution is centered around 2 ($\\mu$).\n",
    "The likelihood function is centered around $6$ which is the only observation we have.\n",
    "\n",
    "The posterior distribution is centred exactly between the prior and the likelihood.\n",
    "In this case $B=0.5$ and therefore prior and data are equally weighted.\n",
    "\n",
    "The _maximum a posteriori probability_ (MAP) estimate is $4$, as it is equal to the mode of the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x[which.max(dnorm(x=x, mean=postMean, sd=sqrt(postVar)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we have $\\mu=2$, $\\sigma=\\tau=1$ and set $\\bar{y}=5.8$ and $n=5$. In this case $\\theta_{MAP}=4.43$ with a range of $(3.21, 5.65)$. The MAP has been shifted towards the MLE as we have more data information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# prior (obviously it does not change)\n",
    "mu <- 2\n",
    "tau <- 1\n",
    "x <- seq(-4,10,0.01)\n",
    "plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,2),\n",
    "    type=\"l\", lty=1, ylab=\"Density\", xlab=expression(theta), main=\"\")\n",
    "    legend(x=\"topleft\", legend=c(expression(pi(theta)),\n",
    "    expression(f(y~\"|\"~theta)), expression(p(theta~\"|\"~y))), lty=1:3)\n",
    "\n",
    "# likelihood with more observations\n",
    "y <- c(6, 5, 6, 4, 7)\n",
    "n <- length(y)\n",
    "sigma <- 1\n",
    "points(x=x, y=dnorm(x=x, mean=mean(y), sd=sigma/n), type=\"l\", lty=2)\n",
    "\n",
    "# posterior with more observations\n",
    "postMean <- ( (sigma^2/n)*mu + tau^2*mean(y) ) / ( (sigma^2/n)*mu + tau^2 )\n",
    "postVar <- ( (sigma^2/n)*tau^2 ) / ( (sigma^2/n) + tau^2 )\n",
    "points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=3)\n",
    "\n",
    "# MAP with more observations\n",
    "map <-  x[which.max(dnorm(x=x, mean=postMean, sd=sqrt(postVar)))]\n",
    "cat(\"MAP:\", map, \"(\", map-3*sqrt(postVar),\",\",map+3*sqrt(postVar),\")\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Monte Carlo sampling\n",
    "\n",
    "<img src=\"Images/MonteCarlo.jpeg\" width=\"600\" height=\"600\" />\n",
    "\n",
    "To derive the posterior distribution we can also draw random samples from it instead of directly calculating its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This procedure is often called _Monte Carlo sampling_ after the city famous for its casinos.\n",
    "\n",
    "In the previous example of the Normal/Normal model with multiple observations, we were able to calculate the posterior mean ($4.43$) and posterior variance ($0.17$).\n",
    "From these parameters, we were able to derive (and plot) the density function, the posterior probability itself.\n",
    "Alternatively, we can randomly sample directly from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Monte Carlo sampling\n",
    "par(mfrow=c(3,1))\n",
    "\n",
    "# posterior\n",
    "x <- seq(2,8,0.01)\n",
    "postMean <- 4.43\n",
    "postVar <- 0.16\n",
    "plot(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=1,\n",
    "    ylab=\"Density\", xlab=expression(theta), main=expression(p(theta~\"|\"~y)),\n",
    "    ylim=c(0,1.2), xlim=c(2,8))\n",
    "\n",
    "# sampling\n",
    "y_sampled_1 <- rnorm(n=50, mean=postMean, sd=sqrt(postVar))\n",
    "hist(y_sampled_1, breaks=20, freq=F, lty=2, col=\"grey\", ylim=c(0,1.2), xlim=c(2,8),\n",
    "    sub=\"50 samples\", main=expression(p(theta~\"|\"~y)), xlab=expression(theta))\n",
    "    points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=1)\n",
    "\n",
    "# more sampling\n",
    "y_sampled_2 <- rnorm(n=1e6, mean=postMean, sd=sqrt(postVar))\n",
    "hist(y_sampled_2, breaks=20, freq=F, lty=2, col=\"grey\", ylim=c(0,1.2), xlim=c(2,8),\n",
    "    sub=\"1e6 samples\", main=expression(p(theta~\"|\"~y)), xlab=expression(theta))\n",
    "points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type=\"l\", lty=1, ylab=\"Density\",\n",
    "    xlab=expression(theta), main=expression(p(theta~\"|\"~y)), sub=\"1e6 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The more sampling we do, the closer our sampled distribution will be to the \"true\" posterior distribution. With $50$ samples the empirical posterior mean is $4.444$ while with $1e6$ samples we have an empirical posterior mean of $4.429$ which is very close to our direct estimate of $4.43$. With $50$ samples the empirical posterior variance is $0.105$ while with $1e6$ samples we have an empirical posterior variance of $0.159$ which is very close to our direct estimate of 0.16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this simple Normal/Normal case, Monte Carlo methods are not strictly necessary since the integral in the denominator of Bayes' theorem can be evaluated in closed form.\n",
    "In these cases, it is preferable to derive a smooth curve rather an a histogram of sampled values, and have the corresponding exact values for the posterior parameters.\n",
    "\n",
    "However, there are cases where, given the choice for the likelihood and prior functions, this integral cannot be evaluated.\n",
    "In these conditions, Monte Carlo methods are to be preferred for estimating, or rather approximating, the posterior distribution.\n",
    "\n",
    "As any sample can be drawn from any posterior regardless of the number of unknown parameters $\\theta$, we have the ability to work on problems with (theoretically) unlimited complexity, at the price of not obtaining an exact form for the posterior and performing a large number of samplings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intended Learning Outcomes\n",
    "\n",
    "At the end of this part you are now be able to:\n",
    "* appreciate the use of Bayesian statistics in life sciences,\n",
    "* formulate and explain Bayesâ€™ theorem,\n",
    "* describe a Normal-Normal model and implement it in R with or without Monte Carlo sampling,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
