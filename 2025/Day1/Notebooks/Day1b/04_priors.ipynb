{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![QMUL](Images/QMUL-logo.jpg)\n",
    "\n",
    "# Introduction to Approximate Bayesian Computation in population genetics\n",
    "\n",
    "## Prior distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intended Learning Outcomes\n",
    "\n",
    "At the end of this part you will be able to:\n",
    "* describe the pros and cons of using different priors (e.g. elicited, conjugate, ...),\n",
    "* evaluate the interplay between prior and posterior distributions,\n",
    "* calculate several quantities of interest from posterior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Prior distributions can\n",
    "* be derived from past information or personal opinions from experts;\n",
    "* be distributed as familiar distribution functions;\n",
    "* bear little information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elicited priors\n",
    "\n",
    "The simplest approach to specify $\\pi(\\theta)$ is to define the collection of $\\theta$ which are possible.\n",
    "\n",
    "Then one can assign some probability to each one of these cases and make sure that they sum to $1$.\n",
    "\n",
    "If $\\theta$ is discrete, this looks like a natural approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatively, we may assume that the prior distribution for $\\theta$ belongs to a parametric distributional family $\\pi(\\theta|\\nu)$.\n",
    "\n",
    "Here we choose $\\nu$ so that $\\pi(\\theta|\\nu)$ closely matches our elicited beliefs.\n",
    "\n",
    "This approach has several advantages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* it reduces the effort to the elicitee (you don't have to decide a probability for each value $\\theta$ can have);\n",
    "* it overcomes the finite support problem (as in the case of the histogram);\n",
    "* it may lead to simplifications in the computation of the posterior (as we will see later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a rule of thumb, for elicited priors, it is recommended to focus on quantiles close to the middle of the distribution (e.g. the $50^{th}$, $25^{th}$ and $75^{th}$) rather than extreme quantiles (e.g. the $95^{th}$ and $5^{th}$).\n",
    "You should also assess the symmetry of your prior.\n",
    "\n",
    "Elicited priors can be updated and reassessed as new information is available.\n",
    "\n",
    "They are very useful for experimental design where some ideas on the nature of the studied system is given in input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conjugate priors\n",
    "\n",
    "When choosing a prior distribution $\\pi(\\theta|\\nu)$ some family distributions will make the calculation of posterior distributions more convenient than others will do.\n",
    "\n",
    "It is possible to select a member of that family that is _conjugate_ with the likelihood $f(y|\\theta)$, so that the posterior distribution $p(\\theta|y)$ belongs to the same distributional family as the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Noninformative priors\n",
    "\n",
    "If no reliable prior information on ${\\theta}$ is available,  can we still employ a Bayesian approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is still appropriate if we find a distribution $\\pi({\\theta})$ that contains \"no information\" about ${\\theta}$, in the sense that it does not favour one value over another.\n",
    "\n",
    "We refer to such a distribution as a _noninformative prior_ for ${\\theta}$.\n",
    "\n",
    "All the information in the posterior will arise from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Hierarchical modelling\n",
    "\n",
    "A posterior distribution is typically obtained with two stages, one for $f({y},{\\theta})$, the likelihood of the data, and one for $\\pi({\\theta}, {\\nu})$, the prior distribution of ${\\theta}$ given a vector of _hyperparameters_ ${\\nu}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If we are uncertain about the values for ${\\nu}$, we need an additional stage, a _hyperprior_ , defining the density\n",
    "distribution of hyperparameters.\n",
    "\n",
    "If we denote this distribution as $h({\\nu})$, then the posterior distribution is\n",
    "\\begin{equation}\n",
    "P({\\theta}|{y}) = \\frac{ \\int f({y}|{\\theta})\\pi({\\theta}|{\\nu})\n",
    "               h({\\nu})d{\\nu} }{ \\int \\int f({y}|{\\theta})\\pi({\\theta}|\n",
    "              {\\nu})h({\\nu})d{\\nu}d{\\theta} } \n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
